"""Streamlit module for purchasing correlation clustering.

README: ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯æ—¢å­˜ã® Streamlit ã‚¢ãƒ—ãƒªã«å¯¾ã—ã€ä½µè²·å•†å“ã®ç›¸é–¢â†’ã‚°ãƒ©ãƒ•â†’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
ã¾ã§ã‚’ãƒ¯ãƒ³ã‚¹ãƒˆãƒƒãƒ—ã§å®Ÿè¡Œã™ã‚‹æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚`render_correlation_category_module()` ã‚’
å‘¼ã³å‡ºã›ã° UI å…¨ä½“ãŒæç”»ã•ã‚Œã€å–å¼•æ˜ç´°ãƒ»ãƒ”ãƒœãƒƒãƒˆãƒ»ç›¸é–¢è¡Œåˆ—ã®ã„ãšã‚Œã‹ã‚’å…¥åŠ›ã¨ã—ã¦
ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨å¯è¦–åŒ–ã€CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚’è¡Œãˆã‚‹ã€‚

æ¨å¥¨åˆæœŸè¨­å®šï¼šLouvain + é–¾å€¤0.3 + æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º3ã€‚SKU ãŒ 1,500 ã‚’è¶…ãˆã¦æç”»ãŒ
é‡ã„ã¨ãã¯ã€Œå£²ä¸Šï¼ˆç™»å ´å›æ•°ï¼‰ä¸Šä½500SKUã€ï¼‹ã€Œã‚¨ãƒƒã‚¸ä¸Šä½30,000ä»¶ã€ç¨‹åº¦ã§ååˆ†å®Ÿå‹™çš„ã€‚

ä¸»ãªé–¢æ•°
----------
* build_matrix: å–å¼•æ˜ç´°ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼/ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³Ã—å•†å“è¡Œåˆ—ã‚’ç”Ÿæˆ
* compute_correlation: Pearson / Spearman / Jaccard ã®ç›¸é–¢ãƒ»é¡ä¼¼åº¦ã‚’è¨ˆç®—
* graph_from_corr: ç›¸é–¢è¡Œåˆ—ã‹ã‚‰ NetworkX ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰
* detect_communities: Louvain ã¾ãŸã¯éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿ã§å•†å“ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æŠ½å‡º
* compute_cluster_metrics: ä¸­å¿ƒæ€§ãƒ»ã‚µãƒãƒ¼ãƒˆãªã©ãƒãƒ¼ãƒ‰/ã‚¯ãƒ©ã‚¹ã‚¿æŒ‡æ¨™ã‚’ç®—å‡º
* recommend_threshold: ç›¸é–¢åˆ†å¸ƒã‹ã‚‰æ¨å¥¨é–¾å€¤ã‚’ææ¡ˆ
* render_correlation_category_module: Streamlit ãƒšãƒ¼ã‚¸æç”»ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ

ä¾‹å¤–æ™‚ã¯ UI ä¸Šã§è­¦å‘Šã‚’å‡ºã—ã€æ¨å¥¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æç¤ºã™ã‚‹ã€‚`make_demo_transactions()` ã‚’
åˆ©ç”¨ã™ã‚Œã°ã‚»ãƒ«ãƒ•ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼ˆA-B-C ã®ä½µè²·ãŒå¼·ãå‡ºã‚‹ç­‰ï¼‰ã‚’ç”Ÿæˆã§ãã‚‹ã€‚
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

import networkx as nx
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
from networkx import Graph
from networkx.algorithms.community import greedy_modularity_communities, modularity
from scipy.cluster import hierarchy
from scipy.spatial.distance import squareform
from sklearn.metrics import pairwise_distances

try:  # python-louvain ã¯ä»»æ„ä¾å­˜ï¼ˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã§ããªã„ç’°å¢ƒå‘ã‘ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
    from community import community_louvain
except ImportError:  # pragma: no cover - optional dependency
    community_louvain = None

from core.plot_utils import apply_elegant_theme, render_plotly_with_spinner


@dataclass
class MatrixBuildResult:
    """å–å¼•æ˜ç´°â†’ãƒ”ãƒœãƒƒãƒˆå¤‰æ›ã®çµæœã€‚"""

    matrix: pd.DataFrame
    support_counts: pd.Series
    total_events: int


def _read_table(upload) -> pd.DataFrame:
    """Streamlit ã® UploadedFile ã‹ã‚‰ DataFrame ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""

    if upload is None:
        return pd.DataFrame()
    name = getattr(upload, "name", "uploaded")
    suffix = Path(name).suffix.lower()
    try:
        if suffix in {".parquet", ".pq"}:
            return pd.read_parquet(upload)
        if suffix in {".xlsx", ".xls"}:
            return pd.read_excel(upload)
        upload.seek(0)
        try:
            return pd.read_csv(upload)
        except UnicodeDecodeError:
            upload.seek(0)
            return pd.read_csv(upload, encoding="cp932")
    finally:
        upload.seek(0)


def make_demo_transactions(n_transactions: int = 200, seed: int = 42) -> pd.DataFrame:
    """ä½µè²·ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒåˆ†ã‹ã‚Šã‚„ã™ã„ãƒ€ãƒŸãƒ¼å–å¼•ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã€‚"""

    rng = np.random.default_rng(seed)
    base_products = list("ABCDEFGHIJ")
    dates = pd.date_range("2024-01-01", periods=90, freq="D")
    records: List[Dict[str, Any]] = []
    for tid in range(n_transactions):
        transaction_id = f"T{tid:05d}"
        user_id = f"U{rng.integers(0, 80):04d}"
        date = rng.choice(dates)
        basket_type = rng.choice(["ABC", "DE", "RANDOM"], p=[0.35, 0.2, 0.45])
        if basket_type == "ABC":
            basket = ["A", "B", "C"]
        elif basket_type == "DE":
            basket = ["D", "E"]
        else:
            basket = list(rng.choice(base_products[5:], size=rng.integers(1, 4), replace=False))
        if rng.random() < 0.1:
            basket.append(rng.choice(base_products))
        for pid in basket:
            qty = int(rng.integers(1, 4))
            price = float(rng.integers(100, 600))
            records.append(
                {
                    "transaction_id": transaction_id,
                    "user_id": user_id,
                    "product_id": pid,
                    "product_name": f"å•†å“{pid}",
                    "qty": qty,
                    "amount": qty * price,
                    "date": date,
                }
            )
    return pd.DataFrame(records)


def build_matrix(
    df: pd.DataFrame,
    id_col: str,
    prod_col: str,
    val_col: Optional[str] = None,
    *,
    binary: bool = True,
) -> MatrixBuildResult:
    """å–å¼•æ˜ç´°ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼/ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³Ã—å•†å“è¡Œåˆ—ã‚’ä½œæˆã™ã‚‹ã€‚"""

    if df.empty:
        return MatrixBuildResult(pd.DataFrame(), pd.Series(dtype=float), 0)
    if id_col not in df.columns or prod_col not in df.columns:
        raise KeyError("æŒ‡å®šã—ãŸåˆ—ãŒãƒ‡ãƒ¼ã‚¿å†…ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")

    work = df[[id_col, prod_col] + ([val_col] if val_col else [])].copy()
    work = work.dropna(subset=[id_col, prod_col])
    work[id_col] = work[id_col].astype(str)
    work[prod_col] = work[prod_col].astype(str)

    value_col = val_col if val_col else "_count"
    if not val_col:
        work[value_col] = 1
    pivot = (
        work.pivot_table(
            index=id_col,
            columns=prod_col,
            values=value_col,
            aggfunc="sum",
            fill_value=0,
        )
        .sort_index()
        .astype(float)
    )
    if binary:
        pivot = (pivot > 0).astype(int)
    support = (pivot > 0).sum(axis=0)
    total_events = pivot.shape[0]
    return MatrixBuildResult(pivot, support, total_events)

def compute_correlation(
    mat: pd.DataFrame,
    method: str = "pearson",
    *,
    sparse: bool = True,
    threshold: float = 0.3,
) -> pd.DataFrame:
    """ç›¸é–¢ãƒ»é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€é–¾å€¤æœªæº€ã‚’0æ‰±ã„ã§ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ã—ãŸè¡Œåˆ—ã‚’è¿”ã™ã€‚"""

    if mat.empty:
        return pd.DataFrame()
    mat = mat.replace([np.inf, -np.inf], np.nan).fillna(0.0)

    method = method.lower()
    if method == "jaccard":
        binary = (mat > 0).astype(int)
        data = binary.values.astype(float)
        dist = pairwise_distances(data.T, metric="jaccard")
        corr = 1 - dist
    elif method == "spearman":
        corr = mat.corr(method="spearman", min_periods=2).values
    else:
        corr = mat.corr(method="pearson", min_periods=2).values
    corr_df = pd.DataFrame(corr, index=mat.columns, columns=mat.columns)
    corr_df = corr_df.fillna(0.0)
    np.fill_diagonal(corr_df.values, 1.0)
    if threshold > 0:
        mask = np.abs(corr_df.values) < threshold
        np.fill_diagonal(mask, False)
        corr_df.values[mask] = 0.0
    return corr_df


def graph_from_corr(
    corr: pd.DataFrame,
    threshold: float,
    *,
    name_map: Optional[Dict[str, str]] = None,
) -> Graph:
    """ç›¸é–¢è¡Œåˆ—ã‹ã‚‰ NetworkX ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""

    G = nx.Graph()
    if corr.empty:
        return G
    cols = corr.columns.tolist()
    for col in cols:
        label = name_map.get(col, col) if name_map else col
        G.add_node(col, label=label)
    values = corr.values
    n = len(cols)
    for i in range(n):
        for j in range(i + 1, n):
            w = float(values[i, j])
            if not np.isfinite(w):
                continue
            if abs(w) < threshold:
                continue
            G.add_edge(
                cols[i],
                cols[j],
                weight=w,
                abs_weight=abs(w),
            )
    return G


def _hierarchical_partition(G: Graph, min_size: int) -> Dict[str, int]:
    nodes = list(G.nodes())
    if len(nodes) <= 1:
        return {node: 0 for node in nodes}
    adj = nx.to_numpy_array(G, nodelist=nodes, weight="abs_weight", nonedge=0.0)
    dist = 1 - adj
    dist = np.clip(dist, 0.0, 1.0)
    if np.allclose(dist, 0.0):
        return {node: 0 for node in nodes}
    condensed = squareform(dist, checks=False)
    Z = hierarchy.linkage(condensed, method="average")
    max_clusters = max(1, len(nodes) // max(min_size, 1))
    labels = hierarchy.fcluster(Z, t=max_clusters, criterion="maxclust")
    return {node: int(labels[i]) for i, node in enumerate(nodes)}


def detect_communities(
    G: Graph,
    method: str = "louvain",
    min_size: int = 3,
    *,
    corr: Optional[pd.DataFrame] = None,
) -> pd.DataFrame:
    """ã‚°ãƒ©ãƒ•ã‹ã‚‰å•†å“ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æŠ½å‡ºã— DataFrame ã§è¿”ã™ã€‚"""

    nodes = list(G.nodes())
    if not nodes:
        return pd.DataFrame(columns=["product_id", "cluster_id", "cluster_size"])
    method = method.lower()
    if method == "louvain" and G.number_of_edges() > 0:
        if community_louvain is not None:
            try:
                partition = community_louvain.best_partition(
                    G, weight="abs_weight", random_state=42
                )
            except ValueError:
                partition = {node: idx for idx, node in enumerate(nodes)}
        else:
            communities = list(greedy_modularity_communities(G, weight="abs_weight"))
            partition = {}
            for idx, comm in enumerate(communities):
                for node in comm:
                    partition[node] = idx
            if not partition:
                partition = {node: idx for idx, node in enumerate(nodes)}
    elif method == "hierarchical":
        partition = _hierarchical_partition(G, min_size=min_size)
    else:
        partition = {node: idx for idx, node in enumerate(nodes)}

    df = pd.DataFrame({"product_id": nodes, "raw_cluster": [partition[n] for n in nodes]})
    df["cluster_size"] = df.groupby("raw_cluster")["product_id"].transform("size")
    valid_clusters = (
        df[df["cluster_size"] >= max(min_size, 1)]
        .groupby("raw_cluster")["product_id"]
        .count()
        .sort_values(ascending=False)
        .index.tolist()
    )
    cluster_id_map = {raw: f"C{idx + 1:02d}" for idx, raw in enumerate(valid_clusters)}
    df["cluster_id"] = df["raw_cluster"].map(cluster_id_map)
    df.loc[df["cluster_size"] < max(min_size, 1), "cluster_id"] = None

    communities_dict: Dict[int, set[str]] = {}
    for node, label in partition.items():
        communities_dict.setdefault(label, set()).add(node)
    communities_list = list(communities_dict.values())

    try:
        if community_louvain is not None and G.number_of_edges() > 0:
            modularity_score = community_louvain.modularity(
                partition, G, weight="abs_weight"
            )
        else:
            modularity_score = modularity(G, communities_list, weight="abs_weight")
    except Exception:
        modularity_score = np.nan
    df.attrs["partition"] = partition
    df.attrs["modularity"] = modularity_score
    df.attrs["cluster_count"] = len(valid_clusters)
    if corr is not None:
        df.attrs["corr_columns"] = corr.columns.tolist()
    return df.sort_values(["cluster_id", "cluster_size"], ascending=[True, False])

def compute_cluster_metrics(
    G: Graph,
    clusters_df: pd.DataFrame,
    *,
    support: Optional[pd.Series] = None,
    total_events: Optional[int] = None,
    corr: Optional[pd.DataFrame] = None,
) -> pd.DataFrame:
    """ãƒãƒ¼ãƒ‰å˜ä½ã®æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ã¨çµåˆã™ã‚‹ã€‚"""

    if clusters_df.empty:
        return pd.DataFrame(
            columns=[
                "product_id",
                "cluster_id",
                "cluster_size",
                "degree_centrality",
                "betweenness_centrality",
                "centrality_score",
                "cluster_avg_corr",
                "support_count",
                "support_rate",
                "lift",
                "is_representative",
            ]
        )

    df = clusters_df.copy()
    deg = nx.degree_centrality(G)
    if len(G) > 200:
        betw = nx.betweenness_centrality(
            G,
            weight="abs_weight",
            normalized=True,
            k=min(200, len(G)),
            seed=42,
        )
    else:
        betw = nx.betweenness_centrality(G, weight="abs_weight", normalized=True)
    df["degree_centrality"] = df["product_id"].map(deg).fillna(0.0)
    df["betweenness_centrality"] = df["product_id"].map(betw).fillna(0.0)
    df["centrality_score"] = 0.6 * df["degree_centrality"] + 0.4 * df["betweenness_centrality"]

    if corr is not None and not corr.empty:
        cluster_avg: Dict[str, float] = {}
        for cid, rows in df[df["cluster_id"].notna()].groupby("cluster_id"):
            nodes = rows["product_id"].tolist()
            sub = corr.loc[nodes, nodes]
            if sub.shape[0] <= 1:
                avg = 0.0
            else:
                vals = sub.values[np.triu_indices(sub.shape[0], k=1)]
                vals = vals[np.isfinite(vals)]
                vals = np.abs(vals)
                avg = float(vals.mean()) if vals.size else 0.0
            for node in nodes:
                cluster_avg[node] = avg
        df["cluster_avg_corr"] = df["product_id"].map(cluster_avg).fillna(0.0)
    else:
        df["cluster_avg_corr"] = 0.0

    if support is not None:
        df["support_count"] = df["product_id"].map(support).fillna(0.0)
        total = float(total_events) if total_events else float(max(df["support_count"].max(), 1))
        df["support_rate"] = df["support_count"] / total
        global_rate = df["support_rate"].mean()
        df["lift"] = df["support_rate"] / global_rate if global_rate > 0 else np.nan
    else:
        df["support_count"] = np.nan
        df["support_rate"] = np.nan
        df["lift"] = np.nan

    df["cluster_rank"] = (
        df[df["cluster_id"].notna()]
        .groupby("cluster_id")["centrality_score"]
        .rank("dense", ascending=False)
    )
    df["is_representative"] = df["cluster_rank"] == 1
    df.loc[df["cluster_id"].isna(), "is_representative"] = False

    df = df.sort_values(
        ["cluster_id", "is_representative", "centrality_score"],
        ascending=[True, False, False],
    )
    return df


def recommend_threshold(corr_values: np.ndarray) -> float:
    """ç›¸é–¢å€¤ã®ä¸Šä½25%ç‚¹ã‚’æ¨å¥¨é–¾å€¤ã¨ã—ã¦è¿”ã™ã€‚"""

    if corr_values.size == 0:
        return 0.3
    vals = np.abs(corr_values[np.isfinite(corr_values)])
    vals = vals[vals < 0.999999]
    if vals.size == 0:
        return 0.3
    return float(np.quantile(vals, 0.75))


def _section_header(title: str, subtitle: str, icon: str = "") -> None:
    icon_html = f"<span class='mck-section-icon'>{icon}</span>" if icon else ""
    st.markdown(
        f"""
        <div class="mck-section-header">
            {icon_html}
            <div>
                <h2>{title}</h2>
                <p class="mck-section-subtitle">{subtitle}</p>
            </div>
        </div>
        """,
        unsafe_allow_html=True,
    )


def _format_product(product_id: str, name_map: Dict[str, str]) -> str:
    name = name_map.get(product_id)
    if name and name != product_id:
        return f"{product_id}ï½œ{name}"
    return product_id

def render_correlation_category_module(
    *,
    plot_config: Optional[Dict[str, Any]] = None,
) -> None:
    """Streamlit ç”¨ã®ä½µè²·ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æç”»ã™ã‚‹ã€‚"""

    _section_header(
        "è³¼è²·ã‚«ãƒ†ã‚´ãƒªæ¢ç´¢",
        "ç›¸é–¢ãŒé«˜ã„å•†å“ç¾¤ã‚’è‡ªå‹•ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ã€ã‚¯ãƒ­ã‚¹ã‚»ãƒ«ã®ã‚¿ãƒã‚’æŠ½å‡ºã€‚",
        icon="ğŸ§®",
    )
    st.caption(
        "ãƒ•ã‚§ãƒ«ãƒŸæ„Ÿè¦šï¼šå°å£²/ECã®ä½µè²·ç›¸é–¢ã¯ Jaccard 0.2ã€œ0.35 ã‚ãŸã‚Šã‹ã‚‰æ„å‘³ãŒå‡ºå§‹ã‚ã¾ã™ã€‚"
    )
    st.info(
        "åˆæœŸæ¨å¥¨ï¼šLouvainï¼‹é–¾å€¤0.30ï¼‹ã‚¯ãƒ©ã‚¹ã‚¿æœ€å°ã‚µã‚¤ã‚º3ã€‚SKUãŒå¤šã„å ´åˆã¯å‡ºç¾ä¸Šä½ã§çµã‚Šã¾ã—ã‚‡ã†ã€‚"
    )
    with st.expander("åˆå¿ƒè€…å‘ã‘ãƒ’ãƒ³ãƒˆ", expanded=False):
        st.write("ãƒ»ç›¸é–¢ãŒå‡ºãªã„å ´åˆã¯é–¾å€¤ã‚’ä¸‹ã’ã‚‹ã‹ã€å¯¾è±¡å•†å“æ•°ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚")
        st.write("ãƒ»äºŒå€¤åŒ–ï¼ˆè³¼å…¥æœ‰ç„¡ï¼‰ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã¨ Jaccard ãŒå®‰å®šã—ã¾ã™ã€‚")
        st.write("ãƒ»SKUãŒå¤šã„å ´åˆã¯å‡ºç¾ä¸Šä½ã«çµã‚Šã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯æœ€å¤§3ä¸‡ã‚¨ãƒƒã‚¸ç¨‹åº¦ã«æŠ‘ãˆã‚‹ã¨è»½å¿«ã§ã™ã€‚")

    side = st.sidebar.container()
    side.subheader("ä½µè²·ã‚¯ãƒ©ã‚¹ã‚¿è¨­å®š")

    input_mode = side.radio(
        "å…¥åŠ›å½¢å¼",
        ("å–å¼•æ˜ç´°", "ãƒ¦ãƒ¼ã‚¶ãƒ¼Ã—å•†å“ãƒ”ãƒœãƒƒãƒˆ", "å•†å“Ã—å•†å“ç›¸é–¢è¡Œåˆ—"),
    )
    data_source = side.radio(
        "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
        ("ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿", "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"),
    )

    uploaded = None
    if data_source == "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        uploaded = side.file_uploader(
            "CSV / Parquet / Excel",
            type=["csv", "tsv", "txt", "parquet", "pq", "xlsx", "xls"],
        )
        if uploaded is None:
            st.info("å·¦ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            return
        df_input = _read_table(uploaded)
    else:
        if input_mode == "å–å¼•æ˜ç´°":
            df_input = make_demo_transactions()
        elif input_mode == "ãƒ¦ãƒ¼ã‚¶ãƒ¼Ã—å•†å“ãƒ”ãƒœãƒƒãƒˆ":
            demo = make_demo_transactions()
            demo_build = build_matrix(demo, "transaction_id", "product_id", binary=True)
            df_input = demo_build.matrix.reset_index().rename(columns={"index": "transaction_id"})
        else:
            demo = make_demo_transactions()
            demo_build = build_matrix(demo, "transaction_id", "product_id", binary=True)
            df_input = compute_correlation(demo_build.matrix, method="jaccard", threshold=0.0)

    if df_input is None or df_input.empty:
        st.warning("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    method_label = side.radio(
        "ç›¸é–¢æŒ‡æ¨™",
        ("Pearson", "Spearman", "Jaccard"),
    )
    method = method_label.lower()

    threshold = side.slider("ç›¸é–¢é–¾å€¤ t (|r|ä»¥ä¸Š)", 0.0, 0.9, 0.3, 0.01)
    min_cluster_size = int(side.slider("æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º k", 2, 20, 3))
    cluster_method = side.selectbox("ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡º", ("Louvain", "éšå±¤ã‚¯ãƒ©ã‚¹ã‚¿"))
    sparse_on = side.checkbox("ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ï¼ˆé–¾å€¤ä¸‹ã‚’0ã«ï¼‰", value=True)
    max_products = int(side.slider("æœ€å¤§å•†å“æ•° (N)", 50, 3000, 500, 50))

    binary_key = "product_corr_binary"
    default_binary = st.session_state.get(binary_key, method == "jaccard")
    binary = side.checkbox(
        "è³¼å…¥æœ‰ç„¡ã§äºŒå€¤åŒ–", value=default_binary, key=binary_key, disabled=method == "jaccard"
    )
    if method == "jaccard":
        binary = True

    name_map: Dict[str, str] = {}
    build_result: Optional[MatrixBuildResult] = None
    corr_df: Optional[pd.DataFrame] = None
    warnings: List[str] = []
    logs: List[str] = []

    if input_mode == "å–å¼•æ˜ç´°":
        columns = df_input.columns.tolist()
        id_candidates = [c for c in columns if "user" in c.lower() or "trans" in c.lower()]
        if not id_candidates:
            id_candidates = columns
        id_col = side.selectbox("é›†è¨ˆè»¸ (user_id / transaction_id)", id_candidates)
        prod_candidates = [c for c in columns if "product" in c.lower() or "sku" in c.lower()]
        if not prod_candidates:
            prod_candidates = [columns[0]]
        prod_col = side.selectbox("å•†å“IDåˆ—", prod_candidates)
        numeric_cols = [c for c in columns if pd.api.types.is_numeric_dtype(df_input[c])]
        val_options = ["<äºŒå€¤>"] + numeric_cols
        val_col = side.selectbox("æ•°é‡/é‡‘é¡åˆ— (ä»»æ„)", val_options, index=0)
        if val_col == "<äºŒå€¤>":
            val_col = None
        if "product_name" in df_input.columns:
            name_map = (
                df_input[[prod_col, "product_name"]]
                .dropna()
                .drop_duplicates(subset=[prod_col])
                .set_index(prod_col)["product_name"]
                .to_dict()
            )
        try:
            build_result = build_matrix(
                df_input,
                id_col=id_col,
                prod_col=prod_col,
                val_col=val_col,
                binary=binary,
            )
        except KeyError as exc:  # pragma: no cover (UIåˆ¶å¾¡ã§ä¿è­·)
            st.error(str(exc))
            return
    elif input_mode == "ãƒ¦ãƒ¼ã‚¶ãƒ¼Ã—å•†å“ãƒ”ãƒœãƒƒãƒˆ":
        columns = df_input.columns.tolist()
        id_col = side.selectbox("è¡Œãƒ©ãƒ™ãƒ«åˆ—", columns)
        matrix = df_input.set_index(id_col)
        matrix.columns = matrix.columns.astype(str)
        matrix = matrix.apply(pd.to_numeric, errors="coerce").fillna(0.0)
        if binary:
            matrix = (matrix > 0).astype(int)
        build_result = MatrixBuildResult(matrix, (matrix > 0).sum(axis=0), matrix.shape[0])
    else:
        corr_df = df_input.copy()
        if corr_df.columns[0] != corr_df.index.name and corr_df.columns[0] not in corr_df.columns[1:]:
            first_col = corr_df.columns[0]
            corr_df = corr_df.set_index(first_col)
        if corr_df.shape[0] != corr_df.shape[1]:
            st.error("ç›¸é–¢è¡Œåˆ—ã¯æ­£æ–¹è¡Œåˆ—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
            return
        if corr_df.columns.tolist() != corr_df.index.tolist():
            corr_df.columns = corr_df.index
        corr_df = corr_df.apply(pd.to_numeric, errors="coerce").fillna(0.0)
        np.fill_diagonal(corr_df.values, 1.0)
        logs.append("ç›¸é–¢è¡Œåˆ—ã‚’ç›´æ¥èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")

    if build_result:
        matrix = build_result.matrix
        support = build_result.support_counts
        total_events = build_result.total_events
        if matrix.shape[1] == 0:
            st.warning("å•†å“åˆ—ãŒæ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚åˆ—æŒ‡å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            return
        if matrix.shape[1] > max_products:
            top_products = support.sort_values(ascending=False).head(max_products).index
            matrix = matrix[top_products]
            support = support[top_products]
            logs.append(f"å•†å“æ•°ãŒå¤šã„ãŸã‚ä¸Šä½ {max_products} SKU ã«çµã‚Šã¾ã—ãŸ (æ®‹ã‚Š {matrix.shape[1]}ä»¶)ã€‚")
        if matrix.shape[1] > 3000:
            warnings.append("å•†å“æ•°ãŒ3,000ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚å¯¾è±¡ã‚’çµã‚‹ã‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚")
        progress = st.progress(0, text="ç›¸é–¢ã‚’è¨ˆç®—ä¸­â€¦")
        corr_df = compute_correlation(
            matrix,
            method=method,
            sparse=sparse_on,
            threshold=threshold if sparse_on else 0.0,
        )
        progress.progress(100, text="ç›¸é–¢è¨ˆç®—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        if not sparse_on and threshold > 0:
            mask = np.abs(corr_df.values) < threshold
            np.fill_diagonal(mask, False)
            corr_df.values[mask] = 0.0
    else:
        support = None
        total_events = None

    if corr_df is None or corr_df.empty:
        st.info("ç›¸é–¢è¨ˆç®—å¯¾è±¡ã®å•†å“ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚æ¡ä»¶ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
        return

    name_map = {k: v for k, v in name_map.items() if k in corr_df.columns}

    G = graph_from_corr(corr_df, threshold=max(threshold, 0.0 if not sparse_on else threshold), name_map=name_map)
    if G.number_of_edges() == 0:
        st.warning("é–¾å€¤æ¡ä»¶ã‚’æº€ãŸã™ã‚¨ãƒƒã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã€‚é–¾å€¤ã‚’ä¸‹ã’ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        rec = recommend_threshold(corr_df.values[np.triu_indices_from(corr_df, k=1)])
        st.caption(f"æ¨å¥¨é–¾å€¤ï¼ˆä¸Šä½25%ç‚¹ï¼‰: ç´„ {rec:.2f}")
        return

    clusters_df = detect_communities(
        G,
        method="louvain" if cluster_method.lower().startswith("louvain") else "hierarchical",
        min_size=min_cluster_size,
        corr=corr_df,
    )
    metrics_df = compute_cluster_metrics(
        G,
        clusters_df,
        support=support,
        total_events=total_events,
        corr=corr_df,
    )

    if name_map:
        metrics_df["product_name"] = metrics_df["product_id"].map(name_map)
    else:
        metrics_df["product_name"] = metrics_df["product_id"]
    assigned = metrics_df[metrics_df["cluster_id"].notna()]
    total_products = max(len(metrics_df["product_id"].unique()), 1)
    coverage = len(assigned["product_id"].unique()) / total_products
    cluster_count = int(clusters_df.attrs.get("cluster_count", assigned["cluster_id"].nunique()))
    avg_size = float(
        assigned.groupby("cluster_id")["product_id"].nunique().mean()
    ) if cluster_count else 0.0
    avg_corr = float(assigned["cluster_avg_corr"].mean()) if not assigned.empty else 0.0
    modularity = float(clusters_df.attrs.get("modularity", np.nan))

    c1, c2, c3, c4, c5 = st.columns(5)
    c1.metric("ã‚¯ãƒ©ã‚¹ã‚¿æ•°", f"{cluster_count}")
    c2.metric("å¹³å‡ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º", f"{avg_size:.1f}")
    c3.metric("ã‚«ãƒãƒ¼ç‡", f"{coverage * 100:.1f}%")
    c4.metric("ã‚¯ãƒ©ã‚¹ã‚¿å†…å¹³å‡ç›¸é–¢", f"{avg_corr:.2f}")
    c5.metric("ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒªãƒ†ã‚£", "{:.2f}".format(modularity) if not np.isnan(modularity) else "â€”")

    st.markdown("---")
    st.subheader("ç›¸é–¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—")
    limit_heatmap = st.checkbox("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ä»£è¡¨å•†å“ã«é™å®š", value=True)
    if limit_heatmap and not assigned.empty:
        top_nodes = (
            assigned.sort_values(["cluster_id", "centrality_score"], ascending=[True, False])
            .groupby("cluster_id")
            .head(5)["product_id"].tolist()
        )
        if top_nodes:
            sub_corr = corr_df.loc[top_nodes, top_nodes]
        else:
            sub_corr = corr_df
    else:
        sub_corr = corr_df
    display_corr = sub_corr.copy()
    display_corr.index = [
        _format_product(pid, name_map) if name_map else pid for pid in display_corr.index
    ]
    display_corr.columns = [
        _format_product(pid, name_map) if name_map else pid for pid in display_corr.columns
    ]
    heat_fig = px.imshow(
        display_corr,
        color_continuous_scale="RdBu_r",
        zmin=-1,
        zmax=1,
        aspect="auto",
    )
    heat_fig.update_layout(height=500)
    heat_fig = apply_elegant_theme(heat_fig, theme=st.session_state.get("ui_theme", "light"))
    render_plotly_with_spinner(heat_fig, config=plot_config or {})

    st.subheader("ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚°ãƒ©ãƒ•")
    max_edges = 30000
    G_draw = G.copy()
    if G_draw.number_of_edges() > max_edges:
        top_edges = sorted(
            G_draw.edges(data=True),
            key=lambda x: x[2].get("abs_weight", 0),
            reverse=True,
        )[:max_edges]
        G_draw = nx.Graph()
        for node, data in G.nodes(data=True):
            G_draw.add_node(node, **data)
        G_draw.add_edges_from([(u, v, d) for u, v, d in top_edges])
        warnings.append(f"ã‚¨ãƒƒã‚¸æ•°ãŒå¤šã„ãŸã‚ä¸Šä½ {max_edges:,} æœ¬ã«åˆ¶é™ã—ã¾ã—ãŸã€‚")
    max_nodes = 250
    if G_draw.number_of_nodes() > max_nodes:
        keep_nodes = (
            metrics_df.sort_values("centrality_score", ascending=False)
            .head(max_nodes)["product_id"].tolist()
        )
        G_draw = G_draw.subgraph(keep_nodes).copy()
        warnings.append(f"ãƒãƒ¼ãƒ‰æ•°ãŒå¤šã„ãŸã‚ä¸­å¿ƒæ€§ä¸Šä½ {max_nodes} ä»¶ã«é™å®šã—ã¦æç”»ã—ã¾ã—ãŸã€‚")

    pos = nx.spring_layout(G_draw, weight="abs_weight", seed=42, k=None)
    edge_x: List[float] = []
    edge_y: List[float] = []
    for u, v, data in G_draw.edges(data=True):
        edge_x += [pos[u][0], pos[v][0], None]
        edge_y += [pos[u][1], pos[v][1], None]
    node_x = [pos[node][0] for node in G_draw.nodes()]
    node_y = [pos[node][1] for node in G_draw.nodes()]
    metrics_idx = metrics_df.set_index("product_id")

    cluster_colors = px.colors.qualitative.G10 + px.colors.qualitative.Safe + px.colors.qualitative.Bold
    color_map = {}
    for idx, cid in enumerate(sorted(assigned["cluster_id"].dropna().unique())):
        color_map[cid] = cluster_colors[idx % len(cluster_colors)]

    node_colors = []
    node_sizes = []
    node_text: List[str] = []
    for node in G_draw.nodes():
        if node in metrics_idx.index:
            row = metrics_idx.loc[node]
            cluster_label = row.get("cluster_id")
            if pd.isna(cluster_label) or not cluster_label:
                cluster_label = "æœªå‰²å½“"
            support_txt = (
                f"<br>æ”¯æŒåº¦: {row['support_rate']:.2%}"
                if not pd.isna(row.get("support_rate"))
                else ""
            )
            node_colors.append(color_map.get(row.get("cluster_id"), "#9ca3af"))
            node_sizes.append(18 + 120 * row.get("centrality_score", 0.0))
            node_text.append(
                f"{_format_product(node, name_map)}"
                f"<br>ã‚¯ãƒ©ã‚¹ã‚¿: {cluster_label}"
                f"<br>ä¸­å¿ƒæ€§: {row.get('centrality_score', 0.0):.2f}"
                f"<br>æ¬¡æ•°: {row.get('degree_centrality', 0.0):.2f}"
                f"<br>åª’ä»‹: {row.get('betweenness_centrality', 0.0):.2f}"
                f"{support_txt}"
            )
        else:
            node_colors.append("#9ca3af")
            node_sizes.append(12)
            node_text.append(_format_product(node, name_map))

    edge_trace = go.Scatter(
        x=edge_x,
        y=edge_y,
        line=dict(width=0.5, color="rgba(100,100,100,0.4)"),
        hoverinfo="none",
        mode="lines",
    )
    node_trace = go.Scatter(
        x=node_x,
        y=node_y,
        mode="markers",
        marker=dict(size=node_sizes, color=node_colors, line=dict(width=1, color="#1f2937")),
        hoverinfo="text",
        text=node_text,
    )
    net_fig = go.Figure(data=[edge_trace, node_trace])
    net_fig.update_layout(
        showlegend=False,
        margin=dict(l=20, r=20, t=20, b=20),
        xaxis=dict(visible=False),
        yaxis=dict(visible=False),
        height=620,
    )
    net_fig = apply_elegant_theme(net_fig, theme=st.session_state.get("ui_theme", "light"))
    render_plotly_with_spinner(net_fig, config=plot_config or {})

    st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿ä¸€è¦§")
    table_cols = [
        "cluster_id",
        "product_id",
        "product_name",
        "centrality_score",
        "degree_centrality",
        "betweenness_centrality",
        "cluster_avg_corr",
        "support_rate",
        "lift",
        "is_representative",
    ]
    table = metrics_df[table_cols].copy()
    table = table.rename(
        columns={
            "cluster_id": "ã‚¯ãƒ©ã‚¹ã‚¿",
            "product_id": "å•†å“ID",
            "product_name": "å•†å“å",
            "centrality_score": "ä¸­å¿ƒæ€§ã‚¹ã‚³ã‚¢",
            "degree_centrality": "æ¬¡æ•°ä¸­å¿ƒæ€§",
            "betweenness_centrality": "åª’ä»‹ä¸­å¿ƒæ€§",
            "cluster_avg_corr": "ã‚¯ãƒ©ã‚¹ã‚¿å¹³å‡ç›¸é–¢",
            "support_rate": "æ”¯æŒåº¦",
            "lift": "ãƒªãƒ•ãƒˆ",
            "is_representative": "ä»£è¡¨",
        }
    )
    table["ä»£è¡¨"] = table["ä»£è¡¨"].map({True: "â˜…", False: ""})
    st.dataframe(table, use_container_width=True)

    st.subheader("ã‚¯ãƒ©ã‚¹ã‚¿åˆ¥ãƒãƒ¼è¡¨ç¤º")
    if not assigned.empty:
        cluster_summary = (
            assigned.groupby("cluster_id")
            .agg(
                å•†å“æ•°=("product_id", "nunique"),
                å¹³å‡ç›¸é–¢=("cluster_avg_corr", "mean"),
                å¹³å‡æ”¯æŒåº¦=("support_rate", "mean"),
            )
            .reset_index()
        )
        bar_fig = px.bar(
            cluster_summary,
            x="cluster_id",
            y="å•†å“æ•°",
            color="å¹³å‡ç›¸é–¢",
            text="å•†å“æ•°",
            hover_data={"å¹³å‡ç›¸é–¢": ":.2f", "å¹³å‡æ”¯æŒåº¦": ":.2%"},
            color_continuous_scale="Blues",
        )
        bar_fig.update_layout(height=420, xaxis_title="ã‚¯ãƒ©ã‚¹ã‚¿", yaxis_title="å•†å“æ•°")
        bar_fig = apply_elegant_theme(bar_fig, theme=st.session_state.get("ui_theme", "light"))
        render_plotly_with_spinner(bar_fig, config=plot_config or {})
    else:
        st.info("æœ€å°ã‚µã‚¤ã‚ºæ¡ä»¶ã‚’æº€ãŸã™ã‚¯ãƒ©ã‚¹ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚é–¾å€¤ã‚„æœ€å°ã‚µã‚¤ã‚ºã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")

    st.subheader("CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
    corr_csv = corr_df.to_csv(index=True, encoding="utf-8-sig")
    edge_list = [
        {"source": u, "target": v, "weight": data.get("weight", 0.0)}
        for u, v, data in G.edges(data=True)
        if abs(data.get("weight", 0.0)) >= threshold
    ]
    edges_df = pd.DataFrame(edge_list)
    clusters_export = metrics_df.copy()
    st.download_button(
        "ç›¸é–¢è¡Œåˆ—ã‚’CSVä¿å­˜",
        data=corr_csv,
        file_name="corr_matrix.csv",
        mime="text/csv",
    )
    st.download_button(
        "ã‚¨ãƒƒã‚¸ãƒªã‚¹ãƒˆã‚’CSVä¿å­˜",
        data=edges_df.to_csv(index=False, encoding="utf-8-sig"),
        file_name="edges.csv",
        mime="text/csv",
    )
    st.download_button(
        "ã‚¯ãƒ©ã‚¹ã‚¿å‰²å½“ã‚’CSVä¿å­˜",
        data=clusters_export.to_csv(index=False, encoding="utf-8-sig"),
        file_name="clusters.csv",
        mime="text/csv",
    )

    st.subheader("ãƒ­ã‚°ãƒ»æ¨å¥¨å€¤")
    for msg in logs:
        st.caption(f"LOG: {msg}")
    for warn in warnings:
        st.warning(warn)

